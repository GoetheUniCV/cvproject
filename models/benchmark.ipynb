{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9blR0NDpn7jdu0OICdviH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoetheUniCV/cvproject/blob/main/models/benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo-qxrzb1bFr"
      },
      "source": [
        "# CNN Model\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0A4WXxX-6rA"
      },
      "source": [
        "##1. Setup\r\n",
        "###1.1 Python Environment\r\n",
        "Almost all required packages are preinstalled, except _kerastuner_. You can install it with `!pip install -U keras-tuner`.\r\n",
        "\r\n",
        "###1.2 Google Drive\r\n",
        "####1.2.1 Structure\r\n",
        "Create the following directory structure in your Drive:\r\n",
        "```\r\n",
        "MyDrive\r\n",
        "└── cvproject\r\n",
        "    ├── COVID-10 Radiography Database\r\n",
        "    │   ├── COVID-19\r\n",
        "    │   ├── NORMAL\r\n",
        "    │   └── Viral Pneumonia\r\n",
        "    └── models\r\n",
        "```\r\n",
        "####1.2.2 Upload COVID-19 Dataset\r\n",
        "The directory _COVID-10 Radiography Database_ must contain the dataset from Kaggle (https://www.kaggle.com/tawsifurrahman/covid19-radiography-database).\r\n",
        "\r\n",
        "___\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knQA0YZ9aUKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933561dc-7da5-48df-f825-0966c04d0e23"
      },
      "source": [
        "!pip install -U keras-tuner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 21.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 22.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.8)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (1.0.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=d51b28a0b702d8f98d942f2dabf31fdd24451dcbae43f2632af928727e17f122\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=7e4bd83dbe8dc7dac6e67195dc3d8b350f8aab54e4856333e091704eb1243159\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoE_CSsQ-4i-"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import kerastuner as kt\r\n",
        "from kerastuner.tuners import RandomSearch\r\n",
        "from kerastuner.engine.hyperparameters import HyperParameter as hp\r\n",
        "from keras.layers import Dense,Dropout,Activation,Add,MaxPooling2D,Conv2D,Flatten\r\n",
        "from keras.models import Sequential \r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.applications import VGG19\r\n",
        "from keras import layers\r\n",
        "from keras.preprocessing import image\r\n",
        "from google.colab import drive\r\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YODzOjnUN8wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b51227-b8d6-408d-ae33-3ee2200cea41"
      },
      "source": [
        "# This mounts your Google Drive to the Colab VM.\r\n",
        "drive.mount('/content/drive', force_remount=True)\r\n",
        "\r\n",
        "# Foldername in your Drive where you have saved the COVID-19 data.\r\n",
        "PROJECT_PATH = '/content/drive/My Drive/cvproject/'\r\n",
        "DATABASE_PATH = f'{PROJECT_PATH}/COVID-19 Radiography Database'\r\n",
        "MODELS_PATH = f'{PROJECT_PATH}/models'\r\n",
        "\r\n",
        "#assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\r\n",
        "\r\n",
        "# now that we've mounted your Drive, this ensures that\r\n",
        "# the Python interpreter of the Colab VM can load\r\n",
        "# python files from within it.\r\n",
        "#sys.path.append(FOLDERPATH)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrWrml9Qcwf0",
        "outputId": "065c7937-508f-4d28-cc5c-861a5803ae43"
      },
      "source": [
        "# Check if database is complete.\r\n",
        "try:\r\n",
        "  if len(os.listdir(f'{DATABASE_PATH}/COVID-19')) == 1143:\r\n",
        "    print('COVID-19 complete.')\r\n",
        "  else:\r\n",
        "    print('COVID-19 incomplete.')\r\n",
        "\r\n",
        "  if len(os.listdir(f'{DATABASE_PATH}/NORMAL')) == 1341:\r\n",
        "    print('NORMAL complete.')\r\n",
        "  else:\r\n",
        "    print('NORMAL incomplete.')\r\n",
        "\r\n",
        "  if len(os.listdir(f'{DATABASE_PATH}/Viral Pneumonia')) == 1345:\r\n",
        "    print('Viral Pneumonia complete.')\r\n",
        "  else:\r\n",
        "    print('Viral Pneumonia incomplete.')\r\n",
        "except FileNotFoundError:\r\n",
        "  print(\"Warning: Some directories are missing!\")\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COVID-19 complete.\n",
            "NORMAL complete.\n",
            "Viral Pneumonia complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL_UxCAZAay9",
        "outputId": "a1e480f7-4e5c-4fc4-d8d8-6b1ccd6b0c52"
      },
      "source": [
        "# Show label name and its encoded value:\r\n",
        "class_names =  [\"COVID-19\",\"NORMAL\",\"Viral Pneumonia\"]\r\n",
        "\r\n",
        "for i, class_name in enumerate(class_names):\r\n",
        "    print(f'{class_name}: {i}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COVID-19: 0\n",
            "NORMAL: 1\n",
            "Viral Pneumonia: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixqyCb84dfsb",
        "outputId": "6528f06e-8667-440d-8dfb-8dd37d1b2750"
      },
      "source": [
        "# NOTE: Bildgröße aller Bilder wird auf 244 x 244 geändert. Leider sind die\r\n",
        "#       Covid Bilder sehr klein. Normale und Lungenentzündung sind 1024 x 1024.\r\n",
        "\r\n",
        "# NOTE: removed '= defect_tree ='\r\n",
        "\r\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "    directory=DATABASE_PATH,\r\n",
        "    labels=\"inferred\",\r\n",
        "    label_mode=\"int\",\r\n",
        "    class_names=class_names,\r\n",
        "    color_mode=\"rgb\",\r\n",
        "    batch_size=32,\r\n",
        "    image_size=(244, 244),\r\n",
        "    shuffle=True,\r\n",
        "    seed=123,\r\n",
        "    validation_split=0.2,\r\n",
        "    subset=\"training\",\r\n",
        "    interpolation=\"bilinear\",\r\n",
        "    follow_links=False,\r\n",
        ")\r\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "    directory=DATABASE_PATH,\r\n",
        "    labels=\"inferred\",\r\n",
        "    label_mode=\"int\",\r\n",
        "    class_names=class_names,\r\n",
        "    color_mode=\"rgb\",\r\n",
        "    batch_size=32,\r\n",
        "    image_size=(244, 244),\r\n",
        "    shuffle=True,\r\n",
        "    seed=123,\r\n",
        "    validation_split=0.2,\r\n",
        "    subset=\"validation\",\r\n",
        "    interpolation=\"bilinear\",\r\n",
        "    follow_links=False,\r\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3829 files belonging to 3 classes.\n",
            "Using 3064 files for training.\n",
            "Found 3829 files belonging to 3 classes.\n",
            "Using 765 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUfrw9lFzazz"
      },
      "source": [
        "# Defining the CNNs.\r\n",
        "model_1 = tf.keras.models.Sequential([\r\n",
        "  layers.Conv2D(64, 5, activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Flatten(),\r\n",
        "  layers.Dense(256, activation='relu'),\r\n",
        "  layers.Dense(3, activation= 'softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model_2 = tf.keras.models.Sequential([\r\n",
        "  layers.Conv2D(64, 3, activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Flatten(),\r\n",
        "  layers.Dense(256, activation='relu'),\r\n",
        "  layers.Dense(3, activation= 'softmax')\r\n",
        "])\r\n",
        "\r\n",
        "models = [model_1, model_2]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yrKmQXEzl4c"
      },
      "source": [
        "# Avoid overfitting...\r\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZyUWjxLzmkE"
      },
      "source": [
        "# Compile the models.\r\n",
        "for model in models:  \r\n",
        "  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkiQy_vlzoXx",
        "outputId": "8640a7c6-fd38-4783-ca07-98704fb36798"
      },
      "source": [
        "# Fit the model.\r\n",
        "histories = []\r\n",
        "for model in models:\r\n",
        "  histories.append(model.fit(train_data,validation_data= test_data,batch_size=32,epochs = 20,callbacks=[early]))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "35/96 [=========>....................] - ETA: 8:14 - loss: 11645.0968 - accuracy: 0.4336"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFsJl2kcz0al",
        "outputId": "155d87d0-b844-4c49-d63e-2254ac52ea69"
      },
      "source": [
        "# Evaluate the model.\r\n",
        "for i, model in enumerate(models):\r\n",
        "  print(f'Evaluation of model {i}:')\r\n",
        "  model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 9s 266ms/step - loss: 0.1870 - accuracy: 0.9399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18702074885368347, 0.9398692846298218]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOWAad4uIpaO"
      },
      "source": [
        "#plotting training values\r\n",
        "sns.set()\r\n",
        "\r\n",
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "epochs = range(1, len(loss) + 1)\r\n",
        "\r\n",
        "#accuracy plot\r\n",
        "plt.plot(epochs, acc, color='green', label='Training Accuracy')\r\n",
        "plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "#loss plot\r\n",
        "plt.plot(epochs, loss, color='pink', label='Training Loss')\r\n",
        "plt.plot(epochs, val_loss, color='red', label='Validation Loss')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hSb6Qn-Ip1W"
      },
      "source": [
        "#train data accuracy and loss\r\n",
        "plt.plot(history.history['val_loss'], label = 'training loss')\r\n",
        "plt.plot(history.history['val_accuracy'], label = 'training accuracy')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcoOziXwIroW"
      },
      "source": [
        "#test data accuracy and loss\r\n",
        "plt.plot(history.history['loss'], label = 'training loss')\r\n",
        "plt.plot(history.history['accuracy'], label = 'training accuracy')\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im53-qPRIuat"
      },
      "source": [
        "#save models as .h5 file\r\n",
        "\r\n",
        "for i, model in enumerate(models):\r\n",
        "  #model_json = model.to_json()\r\n",
        "  #with open(f\"model_{i}.json\", \"w\") as json_file:\r\n",
        "  #  json_file.write(model_json)\r\n",
        "    # serialize weights to HDF5\r\n",
        "\r\n",
        "  model.save(f'{MODELS_PATH}/model_{i}.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMZK1gCoI2hm"
      },
      "source": [
        "#model = keras.models.load_model(f'{MODELS_PATH}/model_2021-01-03.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk84EseISq-m"
      },
      "source": [
        " #model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}